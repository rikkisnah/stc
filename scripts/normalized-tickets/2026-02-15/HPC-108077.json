{
  "ticket": {
    "id": "55650795",
    "key": "HPC-108077",
    "project": {
      "key": "HPC",
      "name": "High Performance Computing"
    },
    "type": "Incident",
    "summary": "AMS | 5612 | 2547XL11DG | 2548XR10AH | FDT level2 failure",
    "priority": "Medium"
  },
  "status": {
    "current": "Resolved",
    "resolution": "Resolved",
    "created": "2026-02-10T01:55:19Z",
    "resolved": "2026-02-10T19:11:53Z",
    "updated": "2026-02-12T10:58:28Z"
  },
  "people": {
    "assignee": null,
    "reporter": "jirasd-gear-compute-vmi-notifier-eu-amsterdam-1",
    "watchers": 1
  },
  "location": {
    "region": null,
    "availability_domain": null,
    "building": null,
    "rack_type": null,
    "host_serial": "2547XL11DG",
    "rack_serial": null
  },
  "labels": [
    "2547XL11DG",
    "2549XL80A3",
    "AMS-CPV",
    "CPV-FDT-level2",
    "CPV-NaN",
    "CPV-Triage",
    "GPU_V5_X11-2C_32000_S.02",
    "ams4-qfab-3-400",
    "bldg4-block11-400",
    "sk-5446b803-db45-416d-aa88-77ec39ee8c31",
    "triage-rules-engine"
  ],
  "links": [
    {
      "key": "HPC-98653",
      "relation": "relates to",
      "status": "Resolved",
      "summary": "AMS | ad1 | CPV Job Failure | BM.GPU.B300.8 | initial_setup"
    },
    {
      "key": "HPC-98745",
      "relation": "relates to",
      "status": "Resolved",
      "summary": "AMS | ad1 | CPV Job Failure | BM.GPU.B300.8 | gpuburn_short"
    },
    {
      "key": "HPC-99978",
      "relation": "relates to",
      "status": "Resolved",
      "summary": "AMS | ad1 | CPV Job Timeout | BM.GPU.B300.8 | gpuburn_short"
    },
    {
      "key": "HPC-101455",
      "relation": "relates to",
      "status": "Resolved",
      "summary": "AMS | ad1 | CPV Job Failure | BM.GPU.B300.8 | gpuburn_short"
    },
    {
      "key": "HPC-101740",
      "relation": "relates to",
      "status": "Resolved",
      "summary": "AMS | 5612 | 2547XL11DG | 2548XR10AH | FDT level2 failure"
    },
    {
      "key": "HPC-105791",
      "relation": "relates to",
      "status": "Resolved",
      "summary": "AMS | ad1 | CPV Job Failure | BM.GPU.B300.8 | dcgm_level_4"
    },
    {
      "key": "HPC-105901",
      "relation": "relates to",
      "status": "Resolved",
      "summary": "AMS | ad1 | CPV Job Failure | BM.GPU.B300.8 | dcgm_level_4"
    },
    {
      "key": "HPC-108473",
      "relation": "relates to",
      "status": "Resolved",
      "summary": "AMS | ad1 | CPV Job Failure | BM.GPU.B300.8 | fdt_level2"
    },
    {
      "key": "HPC-109725",
      "relation": "relates to",
      "status": "Resolved",
      "summary": "AMS | ad1 | CPV Job Failure | BM.GPU.B300.8 | nccl_allreduce_perf_single"
    }
  ],
  "sla": {
    "time_to_first_response": "2h 11m",
    "time_to_resolve": "17h 16m"
  },
  "description": "FDT level 2 failure.\nVerify the GPU errors in the job log.\nCreate a RHS ticket with the FDT logs to request an action plan to address the failures.\nIf necessary, open a DO ticket with the RHS action plan for repair. Then soft-reset the instance before resolving this ticket to re-test.\n\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_0 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_0 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_1 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_1 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_10 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_10 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_11 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_11 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_12 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_12 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_13 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_13 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_14 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_14 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_16 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_16 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_17 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_17 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_18 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_18 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_19 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_19 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_20 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_20 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_21 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_21 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_7 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_7 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_8 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_8 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_9 | ERROR: get Cable Speed: 400G, expect_cable_speed: IB-XDR\nDGX-000000000129 | CX8IBconnectivity | connectivity | | GPU, PCIE, Nvlink, I2C | mlx5_9 | ERROR: get Cable Width: 4x, expect_cable_width: 2x\nDGX-000000000009 | ibstress | ibstress | | Infiniband | | mlx5_5(0000:1d:00.3) in LinkLayerMode.IB mode incompatible with mlx5_8(0000:2f:00.1) in LinkLayerMode.ETH mode. Check if IB cards are configured correctly.\n\nFDT level 2 log files:\nhttps://objectstorage.eu-amsterdam-1.oraclecloud.com/p/LRx5P6E2Zp7x057kGZIbkRA3mEg_cr3jPvSN-fAqdL1F0v-LchS8MR5hW9RDD03T/n/hpc/b/Debug/o/2547XL11DG/2547XL11DG-nvidia-bug-report-2026-02-09-234112.log.gz\nhttps://objectstorage.eu-amsterdam-1.oraclecloud.com/p/LRx5P6E2Zp7x057kGZIbkRA3mEg_cr3jPvSN-fAqdL1F0v-LchS8MR5hW9RDD03T/n/hpc/b/Debug/o/fdt_level2/d0feb92d-6551-4791-8bef-e10dd618876d/logs-20260210-001150.tgz\nhttps://hpc.objectstorage.eu-amsterdam-1.oci.customer-oci.com/p/uaoYlph0vQ58rY4OOkFk2j2iXIqC8MzwGcrmWf3XClOBbYKuFKKAPwcRCs3qJuCV/n/hpc/b/JobResults/o/raw/fdt_level2_d0feb92d-6551-4791-8bef-e10dd618876d_AllResults.tgz.gz\n\n*Job ID:* d0feb92d-6551-4791-8bef-e10dd618876d\n*Test Name:* fdt_level2\n*Asset ID:* 2547XL11DG\n*Need help?:* Submit a ticket to the HPC queue\n\nHealth Check Result JSON. Check the Uploaded Log Bundle for Downloadable Log File:\n\nTicket created after rules engine evaluated rules for test : fdt_level2\n\nRe-run 0 times in past 12 hours\nReset 1 times in past 12 hours\nRepairScript invoked 0 times in past 24 hours\nCreateHpcTicket invoked 0 times in past 24 hours",
  "comments": [
    {
      "id": "357554263",
      "author": "jirasd-gear-compute-vmi-notifier-eu-amsterdam-1",
      "created": "2026-02-10T01:55:21Z",
      "body": "_This is comment added by a Jira Automation Workflow (NOT a human being)_\n\nRules of Engagement for Compute HPC/GPU Support\n\n**Rule 1: Create an incident and choose the right component**\nPlease create an incident(not service request) for any support request and ensure that you select the appropriate component for accurate assistance.\n\n**Rule 2: No Compute support for GitHub stacks**\nWe do not provide support for issues related to the GitHub test stacks. Tickets related to those will be closed without further action.\nGitHub test stacks repository: [https://github.com/oracle-quickstart/oci-hpc/tree/master/playbooks](https://github.com/oracle-quickstart/oci-hpc/tree/master/playbooks). Reach out to the Solutions Architects in #oci-hpc-int\n\n**Rule 3: Support for Compute HPC Images only**\nWe offer support exclusively for Compute HPC Images. Please refer to the following link to access the supported Compute HPC Images: [Compute HPC Images](https://objectstorage.eu-frankfurt-1.oraclecloud.com/p/_601lOOXFu5ordAFijwzLPduQ_3To16jFrej0JmxEdz1iKkr2BzqBudhqQ9uikLg/n/hpc/b/exported-compute-hpc-images/o/)\n\n**Rule 4: Provide OCIDs for every issue**\nWhen reporting an issue, please provide the relevant OCIDs (Instance OCIDs, Cluster Network OCIDs) associated with the problem. This information will help us investigate and resolve the issue more effectively.\n\n**Rule 5: Include SOSReports and NVIDIA debug logs for GPU issues**\nFor GPU-related issues, please include the SOSReports logs and NVIDIA debug logs. These logs are valuable for troubleshooting. Please follow the instructions below to obtain them:\n- To collect SOSReports logs, run the command `sudo sosreport`.\n- To collect NVIDIA debug logs, execute the command `sudo nvidia-bug-report.sh`.\n\n**Rule 6: Stay positive! We are committed to solving all your problems**\nWe are here to provide comprehensive support and solve all the challenges you encounter. Stay positive, and we will assist you every step of the way."
    },
    {
      "id": "357854156",
      "author": "Lazaro Morales Pino",
      "created": "2026-02-10T19:11:53Z",
      "body": "Closing to retest."
    }
  ]
}